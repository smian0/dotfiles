#!/usr/bin/env python3
# /// script
# requires-python = ">=3.11"
# dependencies = [
#     "agno",
#     "click",
#     "pydantic",
# ]
# ///
"""
Agno Workflow: Automatic Chunking ‚Üí Graphiti Ingestion

This workflow implements LLM-native automatic chunking that:
1. Analyzes document structure and generates optimal chunking strategy
2. Applies strategy to split document and extract metadata
3. Ingests enriched chunks to Graphiti knowledge graph via MCP

Architecture:
- Strategy Generator Agent: Analyzes structure, generates chunking rules
- Chunk Processor Agent: Applies rules, extracts metadata (titles, summaries, topics)
- Graphiti Ingestion Agent: Uses MCP tools to add episodes to knowledge graph

Usage:
    # Ingest document to Graphiti
    python graphiti_ingestion_workflow.py ingest ~/news-2025-09-13.md --group-id "news_sept_2025"

    # Analyze chunking strategy only (no ingestion)
    python graphiti_ingestion_workflow.py analyze-strategy ~/news-2025-09-13.md
"""

import asyncio
import json
import os
from pathlib import Path
from typing import Any, Dict, List

import click
from pydantic import BaseModel, Field

from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.ollama import Ollama
from agno.workflow import Step, StepInput, StepOutput, Workflow

# Disable telemetry
os.environ["AGNO_TELEMETRY"] = "false"


# ============================================================================
# Pydantic Models for Structured Outputs
# ============================================================================


class ChunkingStrategy(BaseModel):
    """Strategy for chunking a document, generated by LLM analysis.

    Keep schemas flat - use Dict[str, Any] instead of nested models.
    LLM can return complex nested JSON, but Pydantic validation is simpler with flat types.
    """

    document_type: str = Field(description="Type of document (news, code, medical, etc.)")
    confidence: float = Field(description="Confidence score 0.0-1.0")
    boundary_patterns: List[Dict[str, Any]] = Field(
        description="List of boundary pattern objects with pattern, type, description"
    )
    split_rules: Dict[str, Any] = Field(
        description="Rules for splitting document into chunks"
    )
    metadata_extraction: Dict[str, Any] = Field(
        description="Rules for extracting metadata from each chunk"
    )
    schema_org_type: str = Field(
        default="", description="Optional Schema.org type classification"
    )


class EnrichedChunk(BaseModel):
    """A single chunk with extracted metadata."""

    chunk_id: int = Field(description="Sequential chunk identifier")
    title: str = Field(description="Extracted title or headline")
    summary: str = Field(description="Brief summary of chunk content")
    topics: List[str] = Field(description="Extracted topics or keywords")
    content: str = Field(description="Raw chunk content")


class ProcessedChunks(BaseModel):
    """Collection of processed chunks with metadata."""

    chunks: List[EnrichedChunk] = Field(description="List of enriched chunks")
    total_chunks: int = Field(description="Total number of chunks generated")
    strategy_used: str = Field(description="Document type that determined strategy")


class IngestionResult(BaseModel):
    """Result of Graphiti ingestion process."""

    chunks_ingested: int = Field(description="Number of chunks successfully ingested")
    chunks_failed: int = Field(description="Number of chunks that failed")
    group_id: str = Field(description="Graphiti group ID used")
    status: str = Field(description="Overall status: success, partial, failed")


# ============================================================================
# Agent Definitions
# ============================================================================


def create_strategy_generator_agent() -> Agent:
    """
    Strategy Generator Agent: Analyzes document structure and generates chunking rules.

    Uses kimi-k2-thinking:cloud for best structural analysis capabilities.
    Returns structured ChunkingStrategy with boundary patterns and metadata rules.
    """
    return Agent(
        name="Strategy Generator",
        model=Ollama(id="kimi-k2-thinking:cloud", host="http://localhost:11434"),
        description="Analyzes document structure and generates optimal chunking strategy",
        instructions="""You are an expert document structure analyst.

Analyze the provided document sample and generate a comprehensive chunking strategy.

Your analysis should identify:
1. Document type (news, technical doc, code, medical, legal, etc.)
2. Natural boundary markers (headlines, sections, timestamps, code blocks)
3. Optimal splitting patterns (regex or descriptions)
4. Metadata extraction rules (how to extract titles, summaries, topics)

Return a ChunkingStrategy JSON object with:
- document_type: Classification of document
- confidence: Your confidence in this analysis (0.0-1.0)
- boundary_patterns: List of patterns that mark chunk boundaries
- split_rules: Dictionary with splitting instructions
- metadata_extraction: Dictionary with rules for extracting title, summary, topics
- schema_org_type: Optional Schema.org type (e.g., "NewsArticle", "HowTo")

Examples:
- News briefing: Detect headlines with ** markers, split on \\n\\n(?=\\*\\*)
- Code: Detect function/class definitions, split on def/class keywords
- Technical docs: Detect ## headings, split on heading markers

Be specific and actionable in your rules.""",
        output_schema=ChunkingStrategy,
        debug_mode=True,
    )


def create_chunk_processor_agent() -> Agent:
    """
    Chunk Processor Agent: Applies strategy to split document and extract metadata.

    Uses glm-4.6:cloud for fast bulk processing.
    Returns ProcessedChunks with enriched metadata for each chunk.
    """
    return Agent(
        name="Chunk Processor",
        model=Ollama(id="glm-4.6:cloud", host="http://localhost:11434"),
        description="Applies chunking strategy and extracts metadata from each chunk",
        instructions="""You are a document processing specialist.

You will receive:
1. A chunking strategy (with split rules and metadata extraction rules)
2. The full document content

Your task:
1. Apply the split rules to divide the document into chunks
2. For each chunk, extract metadata according to the rules:
   - title: Main heading or first line
   - summary: Brief description (1-2 sentences)
   - topics: Key keywords or themes (3-5 topics)
3. Assign sequential chunk_id starting from 1

Return a ProcessedChunks JSON object with:
- chunks: List of EnrichedChunk objects
- total_chunks: Count of chunks
- strategy_used: The document_type from the strategy

Quality guidelines:
- Ensure clean chunk boundaries (no mid-sentence splits)
- Titles should be concise (5-10 words)
- Summaries should capture key points
- Topics should be specific and relevant

If the strategy's split rules are unclear, use your best judgment to create semantic chunks.""",
        output_schema=ProcessedChunks,
        debug_mode=True,
    )


def create_graphiti_ingestion_agent() -> Agent:
    """
    Graphiti Ingestion Agent: Uses MCP tools to add chunks to knowledge graph.

    Two-model pattern:
    - Main model (glm-4.6:cloud): Excellent tool calling for MCP
    - Parser model (gpt-oss:120b-cloud): Reliable JSON parsing

    Note: MCP tools should be configured separately via Graphiti MCP connection.
    """
    return Agent(
        name="Graphiti Ingester",
        model=Ollama(id="glm-4.6:cloud", host="http://localhost:11434"),
        description="Ingests enriched chunks to Graphiti knowledge graph using MCP tools",
        instructions="""You are a knowledge graph ingestion specialist.

You will receive enriched chunks with metadata (title, summary, topics, content).

Your task:
For each chunk, call the Graphiti MCP tool `mcp__graphiti__add_memory` with:
- name: Document name + chunk title
- episode_body: Formatted episode with metadata + content
- group_id: Provided group ID
- source: "agentic_chunk"
- source_description: "AI-enriched chunk from automatic chunking workflow"

Episode body format:
```
Document: [document name]
Section: [chunk title]
Summary: [chunk summary]
Topics: [comma-separated topics]

------------------------------------------------------------
CONTENT:
[chunk content]
```

Report on ingestion status:
- Number of chunks successfully ingested
- Any failures and reasons
- Overall status (success/partial/failed)

Handle errors gracefully - if one chunk fails, continue with remaining chunks.""",
        debug_mode=True,
    )


# ============================================================================
# Custom Transformation Functions
# ============================================================================


def format_strategy_for_processing(step_input: StepInput, session_state: dict) -> StepOutput:
    """
    Transform: Extract strategy and prepare input for chunk processor.

    Takes strategy from Strategy Generator Agent and formats it for
    the Chunk Processor Agent along with the original document.
    """
    strategy_content = step_input.previous_step_content

    # Extract document path from session state
    document_path = session_state.get("document_path", "")

    # Read full document
    with open(document_path, 'r', encoding='utf-8') as f:
        full_document = f.read()

    formatted_input = f"""CHUNKING STRATEGY:
{strategy_content}

FULL DOCUMENT TO PROCESS:
{full_document}

Apply the strategy's split rules and metadata extraction rules to generate enriched chunks."""

    return StepOutput(content=formatted_input)


def format_chunks_for_ingestion(step_input: StepInput, session_state: dict) -> StepOutput:
    """
    Transform: Format processed chunks for Graphiti ingestion.

    Adds document context and group_id from session state.
    """
    chunks_content = step_input.previous_step_content

    # Extract metadata from session state
    document_path = session_state.get("document_path", "unknown.md")
    group_id = session_state.get("group_id", "auto_chunking")
    document_name = Path(document_path).stem

    formatted_input = f"""DOCUMENT METADATA:
- Name: {document_name}
- Path: {document_path}
- Group ID: {group_id}

PROCESSED CHUNKS:
{chunks_content}

Ingest each chunk to Graphiti using the mcp__graphiti__add_memory tool.
Format episode_body as specified in your instructions."""

    return StepOutput(content=formatted_input)


# ============================================================================
# Workflow Definition
# ============================================================================


def create_graphiti_workflow(debug: bool = False) -> Workflow:
    """
    Create the complete Graphiti ingestion workflow.

    Sequential steps:
    1. Generate Strategy ‚Üí Strategy Generator Agent
    2. Transform Strategy ‚Üí Custom function
    3. Process Chunks ‚Üí Chunk Processor Agent
    4. Transform Chunks ‚Üí Custom function
    5. Ingest to Graphiti ‚Üí Graphiti Ingestion Agent

    Args:
        debug: Enable debug mode for detailed step outputs

    Returns:
        Configured Workflow ready for execution
    """
    # Create agents
    strategy_agent = create_strategy_generator_agent()
    processor_agent = create_chunk_processor_agent()
    ingestion_agent = create_graphiti_ingestion_agent()

    # Define workflow
    workflow = Workflow(
        name="Graphiti Knowledge Graph Ingestion",
        description="Automatic chunking with LLM-native strategy generation ‚Üí Graphiti ingestion",
        db=SqliteDb(db_file="tmp/graphiti_workflow.db"),
        steps=[
            Step(
                name="generate_strategy",
                agent=strategy_agent,
                description="Analyze document and generate chunking strategy",
            ),
            Step(
                name="format_for_processing",
                executor=format_strategy_for_processing,
                description="Prepare strategy and document for chunk processor",
            ),
            Step(
                name="process_chunks",
                agent=processor_agent,
                description="Apply strategy and extract chunk metadata",
            ),
            Step(
                name="format_for_ingestion",
                executor=format_chunks_for_ingestion,
                description="Prepare chunks with metadata for Graphiti",
            ),
            Step(
                name="ingest_to_graphiti",
                agent=ingestion_agent,
                description="Ingest enriched chunks to knowledge graph",
            ),
        ],
        debug_mode=debug,
    )

    return workflow


# ============================================================================
# CLI Interface
# ============================================================================


@click.group()
def cli():
    """
    Graphiti Knowledge Graph Ingestion Workflow

    Automatic chunking with LLM-native strategy generation.
    """
    pass


@cli.command()
@click.argument('file_path', type=click.Path(exists=True))
@click.option(
    '--group-id',
    default='auto_chunking',
    help='Graphiti group ID for ingested episodes'
)
@click.option(
    '--debug',
    is_flag=True,
    help='Enable debug mode (shows all step outputs)'
)
def ingest(file_path: str, group_id: str, debug: bool):
    """
    Ingest document into Graphiti knowledge graph.

    This performs the complete workflow:
    1. Analyzes document structure
    2. Generates optimal chunking strategy
    3. Splits document into semantic chunks
    4. Extracts metadata (titles, summaries, topics)
    5. Ingests to Graphiti via MCP tools

    Example:
        python graphiti_ingestion_workflow.py ingest ~/news-2025-09-13.md --group-id "news_sept"
    """
    async def run_workflow():
        # Read document sample for strategy generation
        with open(file_path, 'r', encoding='utf-8') as f:
            full_content = f.read()
            document_sample = full_content[:2000]  # First 2000 chars for analysis

        # Create workflow
        workflow = create_graphiti_workflow(debug=debug)

        # Format input for strategy agent
        strategy_prompt = f"""Analyze this document sample and generate a chunking strategy.

DOCUMENT PATH: {file_path}

DOCUMENT SAMPLE (first 2000 chars):
{document_sample}

Return a ChunkingStrategy JSON with boundary_patterns, split_rules, and metadata_extraction rules."""

        click.echo(f"\n{'='*60}")
        click.echo(f"üöÄ GRAPHITI INGESTION WORKFLOW")
        click.echo(f"{'='*60}")
        click.echo(f"üìÑ Document: {Path(file_path).name}")
        click.echo(f"üîñ Group ID: {group_id}")
        click.echo(f"üêõ Debug Mode: {debug}")
        click.echo(f"{'='*60}\n")

        # Run workflow with session_state containing document metadata
        result = await workflow.arun(
            input=strategy_prompt,
            session_state={
                "document_path": file_path,
                "document_sample": document_sample,
                "group_id": group_id,
            }
        )

        if debug:
            click.echo("\n" + "="*60)
            click.echo("üìä WORKFLOW RESULT")
            click.echo("="*60)
            await workflow.aprint_response(show_step_details=True)
        else:
            click.echo("\n‚úÖ Workflow completed successfully!")
            click.echo(f"\nFinal result:\n{result}")

    # Run async workflow
    asyncio.run(run_workflow())


@cli.command()
@click.argument('file_path', type=click.Path(exists=True))
def analyze_strategy(file_path: str):
    """
    Analyze document and generate chunking strategy only (no ingestion).

    Useful for testing strategy generation without committing to full ingestion.

    Example:
        python graphiti_ingestion_workflow.py analyze-strategy ~/news-2025-09-13.md
    """
    async def run_analysis():
        # Read document sample
        with open(file_path, 'r', encoding='utf-8') as f:
            document_sample = f.read()[:2000]

        # Create only strategy agent
        strategy_agent = create_strategy_generator_agent()

        # Run analysis
        strategy_prompt = f"""Analyze this document sample and generate a chunking strategy.

DOCUMENT PATH: {file_path}

DOCUMENT SAMPLE (first 2000 chars):
{document_sample}

Return a ChunkingStrategy JSON with boundary_patterns, split_rules, and metadata_extraction rules."""

        click.echo(f"\n{'='*60}")
        click.echo(f"üîç STRATEGY ANALYSIS")
        click.echo(f"{'='*60}")
        click.echo(f"üìÑ Document: {Path(file_path).name}")
        click.echo(f"{'='*60}\n")

        result = await strategy_agent.arun(strategy_prompt)

        click.echo("\nüìã GENERATED STRATEGY:")
        click.echo("="*60)

        # Pretty print strategy
        try:
            strategy_json = json.loads(result.content)
            click.echo(json.dumps(strategy_json, indent=2))
        except:
            click.echo(result.content)

        click.echo("\n‚úÖ Strategy analysis complete!")

    # Run async analysis
    asyncio.run(run_analysis())


if __name__ == "__main__":
    cli()
